# -*- coding: utf-8 -*-
"""HW2_The_Perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Z7GvBFe5Yem-J5afyvz6RS6fMs6yWbu

# **CSCE 5218 / CSCE 4930 Deep Learning**

# **HW1a The Perceptron** (20 pt)
"""

# Get the datasets
#!wget http://huang.eng.unt.edu/CSCE-5218/test.dat
#!wget http://huang.eng.unt.edu/CSCE-5218/train.dat

# Take a peek at the datasets
#!head train.dat
#!head test.dat

"""### Build the Perceptron Model

You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this.
"""

import math
import itertools
import re


# Corpus reader, all columns but the last one are coordinates;
#   the last column is the label
def read_data(file_name):
    f = open(file_name, 'r')

    data = []
    # Discard header line
    f.readline()
    for instance in f.readlines():
        if not re.search('\t', instance): continue
        instance = list(map(int, instance.strip().split('\t')))
        # Add a dummy input so that w0 becomes the bias
        instance = [-1] + instance
        data += [instance]
    return data


def dot_product(array1, array2):
    #TODO: Return dot product of array 1 and array 2
    return sum([array1[i]*array2[i] for i in range(len(array1))])


def sigmoid(x):
    #TODO: Return outpout of sigmoid function on x
    return 1/(1+math.exp(-x))

# The output of the model, which for the perceptron is
# the sigmoid function applied to the dot product of
# the instance and the weights
def output(weights, instance):
    #TODO: return the output of the model
    return sigmoid(dot_product(instance, weights))

# Predict the label of an instance; this is the definition of the perceptron
# you should output 1 if the output is >= 0.5 else output 0
def predict(weights, instance):
    #TODO: return the prediction of the model
    return 1 if (output(weights,instance) > .5) else 0


# Accuracy = percent of correct predictions
def get_accuracy(weights, instances):
    # You do not to write code like this, but get used to it
    correct = sum([1 if predict(weights, instance) == instance[-1] else 0
                   for instance in instances])
    return correct * 100 / len(instances)


# Train a perceptron with instances and hyperparameters:
#       lr (learning rate)
#       epochs
# The implementation comes from the definition of the perceptron
#
# Training consists on fitting the parameters which are the weights
# that's the only thing training is responsible to fit
# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)
#
# Hyperparameters (lr and epochs) are given to the training algorithm
# We are updating weights in the opposite direction of the gradient of the error,
# so with a "decent" lr we are guaranteed to reduce the error after each iteration.
def train_perceptron(instances, lr, epochs):

    #TODO: name this step
#initialization
    weights = [0] * (len(instances[0]))

    for _ in range(epochs):
        for instance in instances:
            #TODO: name these steps
#Generate logits and calculate error
            in_value = dot_product(weights, instance)
            output = sigmoid(in_value)
            error = instance[-1] - output
            #TODO: name these steps
#backpropogate error
            for i in range(0, len(weights)):
                weights[i] += lr * error * output * (1-output) * instance[i]

    return weights

"""## Run it"""

instances_tr = read_data("train.txt")
instances_te = read_data("test.txt")
lr = 0.005
epochs = 5
weights = train_perceptron(instances_tr, lr, epochs)
accuracy = get_accuracy(weights, instances_te)
print(f"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; "
      f"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}")

"""## Questions

Answer the following questions. Include your implementation and the output for each question.

### Question 1

In `train_perceptron(instances, lr, epochs)`, we have the follosing code:
```
in_value = dot_product(weights, instance)
output = sigmoid(in_value)
error = instance[-1] - output
```

Why don't we have the following code snippet instead?
```
output = predict(weights, instance)
error = instance[-1] - output
```

#### TODO Add your answer here (text only)
#The predict function outputs 0s or 1s, IE predictions.  For backpropogation, we need the weights of the model.  For example, .9 is closer of a guess to 1 IE a more confident result, which means the model should not penalize that error as harshly as it is less significant.  With the pure predictions, we would not get such granularity.
### Question 2
Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.

tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with
num_epochs = [5, 10, 20, 50, 100]              # number of epochs
lr = [0.005, 0.01, 0.05]              # learning rate
TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)
of your code.The output should look like the following:
```
# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
[and so on for all the combinations]
```
You will get different results with different hyperparameters.

#### TODO Add your answer here (code and output in the format above)
"""
tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with
num_epochs = [5, 10, 20, 50, 100]              # number of epochs
lr = [0.005, 0.01, 0.05]              # learning rate

print("Question 2:\n")
for ratio in tr_percent:
    data = instances_tr[:int(ratio*.01*len(instances_tr))]
    for epochs in num_epochs:
        for rate in lr:
            weights = train_perceptron(data, rate, epochs)
            accuracy = get_accuracy(weights, instances_te)
            print(f"tr:  {ratio}, epochs:   {epochs}, learning rate: {rate}; Accuracy (test, 100 instances): {accuracy}")
"""tr:  5, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   5, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   5, learning rate: 0.05; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   10, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   10, learning rate: 0.05; Accuracy (test, 100 instances): 74.0
tr:  5, epochs:   20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   20, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   20, learning rate: 0.05; Accuracy (test, 100 instances): 85.0
tr:  5, epochs:   50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  5, epochs:   50, learning rate: 0.01; Accuracy (test, 100 instances): 74.0
tr:  5, epochs:   50, learning rate: 0.05; Accuracy (test, 100 instances): 91.0
tr:  5, epochs:   100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0
tr:  5, epochs:   100, learning rate: 0.01; Accuracy (test, 100 instances): 85.0
tr:  5, epochs:   100, learning rate: 0.05; Accuracy (test, 100 instances): 91.0
tr:  10, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   5, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   5, learning rate: 0.05; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   10, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   10, learning rate: 0.05; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   20, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   20, learning rate: 0.05; Accuracy (test, 100 instances): 78.0
tr:  10, epochs:   50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   50, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   50, learning rate: 0.05; Accuracy (test, 100 instances): 90.0
tr:  10, epochs:   100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  10, epochs:   100, learning rate: 0.01; Accuracy (test, 100 instances): 80.0
tr:  10, epochs:   100, learning rate: 0.05; Accuracy (test, 100 instances): 97.0
tr:  25, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  25, epochs:   5, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  25, epochs:   5, learning rate: 0.05; Accuracy (test, 100 instances): 80.0
tr:  25, epochs:   10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  25, epochs:   10, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  25, epochs:   10, learning rate: 0.05; Accuracy (test, 100 instances): 94.0
tr:  25, epochs:   20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  25, epochs:   20, learning rate: 0.01; Accuracy (test, 100 instances): 70.0
tr:  25, epochs:   20, learning rate: 0.05; Accuracy (test, 100 instances): 98.0
tr:  25, epochs:   50, learning rate: 0.005; Accuracy (test, 100 instances): 77.0
tr:  25, epochs:   50, learning rate: 0.01; Accuracy (test, 100 instances): 92.0
tr:  25, epochs:   50, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  25, epochs:   100, learning rate: 0.005; Accuracy (test, 100 instances): 92.0
tr:  25, epochs:   100, learning rate: 0.01; Accuracy (test, 100 instances): 98.0
tr:  25, epochs:   100, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  50, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  50, epochs:   5, learning rate: 0.01; Accuracy (test, 100 instances): 68.0
tr:  50, epochs:   5, learning rate: 0.05; Accuracy (test, 100 instances): 98.0
tr:  50, epochs:   10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  50, epochs:   10, learning rate: 0.01; Accuracy (test, 100 instances): 83.0
tr:  50, epochs:   10, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  50, epochs:   20, learning rate: 0.005; Accuracy (test, 100 instances): 80.0
tr:  50, epochs:   20, learning rate: 0.01; Accuracy (test, 100 instances): 96.0
tr:  50, epochs:   20, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  50, epochs:   50, learning rate: 0.005; Accuracy (test, 100 instances): 97.0
tr:  50, epochs:   50, learning rate: 0.01; Accuracy (test, 100 instances): 100.0
tr:  50, epochs:   50, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  50, epochs:   100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0
tr:  50, epochs:   100, learning rate: 0.01; Accuracy (test, 100 instances): 100.0
tr:  50, epochs:   100, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  75, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  75, epochs:   5, learning rate: 0.01; Accuracy (test, 100 instances): 71.0
tr:  75, epochs:   5, learning rate: 0.05; Accuracy (test, 100 instances): 99.0
tr:  75, epochs:   10, learning rate: 0.005; Accuracy (test, 100 instances): 71.0
tr:  75, epochs:   10, learning rate: 0.01; Accuracy (test, 100 instances): 92.0
tr:  75, epochs:   10, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  75, epochs:   20, learning rate: 0.005; Accuracy (test, 100 instances): 92.0
tr:  75, epochs:   20, learning rate: 0.01; Accuracy (test, 100 instances): 98.0
tr:  75, epochs:   20, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  75, epochs:   50, learning rate: 0.005; Accuracy (test, 100 instances): 99.0
tr:  75, epochs:   50, learning rate: 0.01; Accuracy (test, 100 instances): 100.0
tr:  75, epochs:   50, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  75, epochs:   100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0
tr:  75, epochs:   100, learning rate: 0.01; Accuracy (test, 100 instances): 100.0
tr:  75, epochs:   100, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
tr:  100, epochs:   5, learning rate: 0.01; Accuracy (test, 100 instances): 80.0
tr:  100, epochs:   5, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   10, learning rate: 0.005; Accuracy (test, 100 instances): 80.0
tr:  100, epochs:   10, learning rate: 0.01; Accuracy (test, 100 instances): 96.0
tr:  100, epochs:   10, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   20, learning rate: 0.005; Accuracy (test, 100 instances): 96.0
tr:  100, epochs:   20, learning rate: 0.01; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   20, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   50, learning rate: 0.005; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   50, learning rate: 0.01; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   50, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   100, learning rate: 0.01; Accuracy (test, 100 instances): 100.0
tr:  100, epochs:   100, learning rate: 0.05; Accuracy (test, 100 instances): 100.0
"""


"""### Question 3
Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:
- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?
- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?
   ```
#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0
#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0
```
- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?
- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?

#### TODO: Add your answer here (code and text)
a: No, it is possible to achieve perfect accuracy with a ratio as low as 25%.  Furthermore, accuracies of over 90 percent are possible with only 5 percent of the data, and 97% with only 10 percent of the data.  
b: Based on the provided learning rates, it is quite likely the second trial was underfit to the training data.  Increasing the number of epochs or the learning rate would likely yield better results.
c: As discussed, there are multiple ways to reach 100%.  One way is with >= 25% of the training set with appropriately tuned learning rate and epochs.  As the training size increases, the configuration of the learning rate and number of epochs can be loosened, as there is a wider window for perfect convergence.  In short, the more data the better, but with careful tuning we can make due with less.
d: No.  If we use a large learning rate and a large number of epochs, we will often find the model overfits to the training data and as a result the test accuracy decreases.  
"""